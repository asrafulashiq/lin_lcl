# @package _global_

defaults:
  - /eval/fine_tune/optimizer@eval.fine_tune.in_train.optimizer: SGD
  - override /eval/fine_tune: deep

data:
  val_dataset: EuroSAT_test
  dataset: EuroSAT_train

support_batch_size: 5
# support_batch_size_fixmatch: 5
support_aug: "few_shot_query"
support_repeat: 5
query_aug: null
# support_aug_fixmatch: "few_shot_query"

sim_epoch: 50
weight_ce: 1
weight_sim: 1

eval:
  fine_tune:
    selftrain: "simsiam"
    # threshold: null
    # use_norm: true # use norm should be true for moco
    model_unlabel: "simsiam"
    # lr_multiplier_backbone: 1.0

    num_unlabel_data: 1000

    # batch_size: 64
    unlabel_aug: "strong_strong"

    in_train:
      target: "system.bolts.ce_simsiam"
      # reset_head: true
      # freeze_backbone: false
      # use_norm: true

      support_aug_simsiam: "strong" #  "few_shot_query"
      support_aug_simsiam_seed: 0
      support_repeat: 5
      support_batch_size: 12

      optimizer:
        _target_: "torch.optim.SGD"
        lr: 0.01
        momentum: 0.9
        dampening: 0
        weight_decay: 1e-5

      trainer_params:
        gpus: 1
        max_epochs: ${sim_epoch}
        checkpoint_callback: false
        logger: false
        weights_summary: null
        progress_bar_refresh_rate: 5
        num_sanity_val_steps: 0
        check_val_every_n_epoch: 5

    unlabel_params:
      batch_size: 32
      unlabel_class_select: null
      trainer_params:
        gpus: 1
        max_epochs: ${sim_epoch}
        check_val_every_n_epoch: 1
        num_sanity_val_steps: 0
        progress_bar_refresh_rate: 0

    simsiam_params:
      hidden_mlp_1: 512
      hidden_mlp_2: 64
      feat_dim: 128
      max_epochs: ${sim_epoch}
      temperature: 0.1 # for softmax-similarity
      use_norm: false
      # optimizer: "sgd"
      # learning_rate: 0.01
      # exclude_bn_bias: false
      # weight_decay: 1e-6
      # optim: "sgd"
      sim_func: "cosine"
      weight_ce: ${weight_ce}
      weight_sim: ${weight_sim}

      optimizer:
        _target_: "torch.optim.SGD"
        lr: 0.01
        momentum: 0.9
        dampening: 0
        weight_decay: 1e-5

      scheduler:
        _target_: "callbacks.custom_callbacks.LRScheduler"
        max_epochs: ${sim_epoch}

    logistic_regression_params:
      freeze_bn: false # batchnorm freeze
      last_k: -1 # unfreeze all
      use_norm: true
      freeze_backbone: true
      optimizer:
        _target_: "torch.optim.SGD"
        lr: 0.01
        momentum: 0.9
        dampening: 0
        weight_decay: 0
      lr_multiplier_backbone: 1.0

launcher:
  time: "6"
